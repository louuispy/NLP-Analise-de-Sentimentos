# NLP-Analise-de-Sentimentos

Faaaaaaaaaaaaala galera, tudo bem com voc√™s? Espero que sim!  
  
Venho aqui compartilhar com voc√™s um projeto que eu fiz recentemente, que consiste basicamente em uma NLP para realizar a an√°lise de sentimentos em coment√°rios do X (antigo Twitter)! Foi um projeto que fiz um tempinho atr√°s, mas que eu ainda n√£o havia compartilhado e documentado passo a passo do projeto aqui com voc√™s, ent√£o aqui est√° todo o projeto! üòä

---

## Constru√ß√£o do projeto

### 1. Preparando a base de dados

De in√≠cio, foram realizadas as importa√ß√µes para as seguintes bibliotecas:
- `pandas`
- `string`
- `spacy`
- `random`
- `seaborn`
- `numpy`
- `re`
  
Agora vamos realmente come√ßar o projeto!

Primeiro, utilizamos a biblioteca pandas para fazer a leitura da base de dados que ser√° utilizada no treinamento da NLP, e imprimir na tela as 5 primeiras linhas do nosso DataFrame.
Para isso, utilizamos o seguinte c√≥digo:

```python
base_treinamento = pd.read_csv('Train50.csv', on_bad_lines='skip', delimiter=';')
base_treinamento.head()
```
Com isso, temos a nossa base de dados atribu√≠da √† vari√°vel `base_treinamento`.

![image](https://github.com/user-attachments/assets/ccd6cd40-cb32-4199-bd93-4ce2958c7a8e)

Essa base de dados cont√©m 50000 registros, que est√£o distribu√≠dos em 5 colunas. Dessas 5 colunas, iremos utilizar apenas `tweet_text` e `sentiment`, que s√£o respectivamente as colunas que cont√©m os dados dos textos e suas respectivas emo√ß√µes.

Para excluir as demais colunas que n√£o ser√£o utilizadas, digitamos:

```python
base_treinamento.drop(['id', 'tweet_date', 'query_used'], axis = 1, inplace = True)
```
Dessa forma, teremos o seguinte DataFrame:

![image](https://github.com/user-attachments/assets/d891101e-b77b-41de-aaf0-f22dcc4cabcd)

Com isso, podemos prosseguir para o pr√©-processamento dos textos!

---

### 2. Pr√©-processamento dos textos

Antes de treinarmos o nosso modelo, precisamos fazer um pr√©-processamento dos textos, uma vez que eles apresentam caracteres especiais, pontua√ß√µes e afins. Iremos limpar os textos, e deix√°-los apenas com palavras.

Para isso, vamos criar a nossa NLP, inst√¢nciando a biblioteca spaCy:
```python
pln = spacy.load('pt_core_news_sm')
pln
```

Al√©m disso, vamos importar do spaCy o m√≥dulo STOP_WORDS.
Mas o que s√£o STOP_WORDS? S√£o basicamente palavras que possuem pouco significado, como preposi√ß√µes, artigos e etc.

```python
from spacy.lang.pt.stop_words import STOP_WORDS
stop_words = STOP_WORDS
```
Agora, vamos finalmente escrever a fun√ß√£o que ir√° realizar todo o pr√©-processamento!

```python
def preprocessamento(texto):
    # Letras min√∫sculas
    texto = texto.lower()
    
    # Nome do usu√°rio
    texto = re.sub(r"@[A-Za-z0-9$-_@.&+]+", ' ', texto)
    
    # URLs
    texto = re.sub(r"https?://[A-Za-z0-9./]+", ' ', texto)
    
    # Espa√ßos em branco
    texto = re.sub(r" +", ' ', texto)
    
    # Emoticons
    lista_emocoes = {':)': 'emocaopositiva',
                    ':d': 'emocaopositiva',
                    ':(': 'emocaonegativa'}
    for emocao in lista_emocoes:
        texto = texto.replace(emocao, lista_emocoes[emocao])
        
    # Lematiza√ß√£o
    documento = pln(texto)
    
    lista = []
    for token in documento:
        lista.append(token.lemma_)
        
    # Stop words e pontua√ß√µes
    lista = [palavra for palavra in lista if palavra not in stop_words and palavra not in string.punctuation]
    lista = ' '.join([str(elemento) for elemento in lista if not elemento.isdigit()])
    
    return lista
```
Basicamente, neste pr√©-processamento, estamos retirando da nossa base de dados todas as letras mai√∫sculas, nomes de usu√°rio, URLs e espa√ßos em branco deixando apenas o mais puro texto. Al√©m disso, substitu√≠mos os emoticons para suas respectivas emo√ß√µes. Por fim, fazemos a lematiza√ß√£o dos textos. A lematiza√ß√£o o prcoesso de representar as palavras atrav√©s do infinitivo dos verbos, ou seja, a gente vai praticamente reduzir a palavra at√© a sua raiz, e retirar todas as inflex√µes.

Ap√≥s aplicarmos o pr√©-processamento utilizando o c√≥digo:

```python
base_treinamento['tweet_text'] = base_treinamento['tweet_text'].apply(preprocessamento)
```

Temos o seguinte DataFrame:


![image](https://github.com/user-attachments/assets/75bf91e5-4612-4842-917f-d917665193b1)

Se voc√™ reparar, n√≥s aplicamos o pr√©-processamento apenas na base de dados de treinamento. Agora, iremos aplicar na base de dados o teste.

```python
base_teste['tweet_text'] = base_teste['tweet_text'].apply(preprocessamento)
```
![image](https://github.com/user-attachments/assets/c667f84a-46f8-4764-ac83-604e31776a37)

---

### 3. Tratamento da classe

Agora vamos fazer o tratamento da classe de nossos textos. No DataFrame, os textos com emo√ß√µes positivas s√£o classificados como 1, e negativas como 0. Vamos realizar um tratamento que vai substituir esses valores por `POSITIVO` e `NEGATIVO`.

```python
base_dados_treinamento_final = []
for texto, emocao in zip(base_treinamento['tweet_text'], base_treinamento['sentiment']):
    if emocao == 1:
        dic = ({'POSITIVO': True, 'NEGATIVO': False})
    elif emocao == 0:
        dic = ({'POSITIVO': False, 'NEGATIVO': True})
    base_dados_treinamento_final.append([texto, dic.copy()])
```

Ao final do c√≥digo, teremos uma lista, com os textos e suas emo√ß√µes:

![image](https://github.com/user-attachments/assets/a8e86d78-4565-45d0-b394-15998c559b56)


---

### 4. Cria√ß√£o do Classificador

Vamos agora realizar o treinamento da nossa NLP!

De in√≠cio, vamos digitar o seguinte c√≥digo:

```python
from spacy.training import Example
modelo = spacy.blank('pt')
categorias = modelo.add_pipe("textcat")
categorias.add_label("POSITIVO")
categorias.add_label("NEGATIVO")
historico = []
```

No geral, n√≥s criamos uma vari√°vel modelo que realiza uma inst√¢ncia do spaCy, e dessa vari√°vel, adicionamos labels 'POSITIVO' e 'NEGATIVO'.
Tamb√©m criamos uma lista vazia, na vari√°vel hist√≥rico, que iremos utilizar logo mais.

Agora, vamos para a parte mais "dif√≠cil" do projeto:

```python
modelo.begin_training()
for epoca in range(5):
    random.shuffle(base_dados_treinamento_final)
    losses = {}
    for batch in spacy.util.minibatch(base_dados_treinamento_final, 512):
        textos = [modelo(texto) for texto, entities in batch]
        annotations = [{'cats': entities} for texto, entities in batch]
        examples = [Example.from_dict(doc, annotation) for doc, annotation in zip(textos, annotations)]
        modelo.update(examples, losses=losses)
        historico.append(losses)
    if epoca % 5 == 0:
        print(losses)
```
Essa parte pode parecer bem complexa, mas vamos explicar! üòÖ
A primeira linha do c√≥digo vai indicar o in√≠cio do treinamento e prepara o modelo em si para receber os dados de treino e as atualiza√ß√µes dos pesos. 

Logo em seguida, criamos um la√ßo de repeti√ß√£o em um `range(5)`, pois iremos treinar nosso modelo em 5 √©pocas. 

Em seguida, temos o c√≥digo `random.shuffle(base_dados_treinamento_final)`, que vai basicamente embaralhar tudo na base de dados de treinamento, evitando assim que o modelo aprenda a ordem dos dados, e apresente resultados com baixa efic√°cia nos testes. 

Ap√≥s isso, criamos uma vari√°vel losses, que recebe um dict. √â nessa vari√°vel que vamos armazenar os erros do modelo durante o treinamento.

Em seguida, no c√≥digo `for batch in spacy.util.minibatch(base_dados_treinamento_final, 512):`, n√≥s dividimos os dados de treinamento em 512 peda√ßos, ou lotes, ou batches, que seria o nome correto. N√≥s faemos para que o modelo realize o pr√©-processamento em um peda√ßo de cada vez. Isso torna o treinamento do modelo mais r√°pido e bem mais eficiente, na pr√°tica.

No trecho seguinte, o `modelo` ser√° aplicado a cada texto presente no batch. Ou seja, ser√° feito um pr√©-processamento nos textos do batch para criar objetos de documento `doc`.

Logo em seguida, criamos uma lista, e dentro dessa lista temos um dict, que ir√° associar a chave `cats` √†s categorias presentes em cada texto no batch.

No trecho seguinte, n√≥s criamos exemplos de treinamento, utilizando os textos j√° pr√©-processados. Isso √© feito a partir de um `doc` e sua respectiva anota√ß√£o. A fun√ß√£o `Example.from_dict` serve para converter esses documentos e anota√ß√µes em um formato compat√≠vel com o treinamento do modelo.

Depois disso, chamamos o m√©todo `update` do modelo para atualizar os pesos do modelo com base nos exemplos fornecidos no trecho anterior, e fazer um c√°lculo das perdas, que ser√° armazenados em `losses`.

Logo em seguida, n√≥s iremos fazer um `append` dos erros na lista `hist√≥rico`.

Por fim, ap√≥s as 5 √©pocas, imprimos na tela o dicion√°rio de perdas, apra assim, monitorarmos o progresso do treinamento da NLP.

---

E √© isso gente, conclu√≠mos nossa NLP! Espero que tenham gostado! üöÄ
Caso voc√™s queiram aprender mais sobre, recomendo muito os cursos do Jonas Granatyr, da Udemy! ^^
